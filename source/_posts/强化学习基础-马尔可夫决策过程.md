---
title: 强化学习基础-马尔可夫决策过程
categories: 强化学习
date: 2023-10-7 22:50:00
mathjax: true
---

原网页：[链接](https://hrl.boyuai.com/chapter/1/马尔可夫决策过程)

Pdf课件：[链接](https://boyuai.oss-cn-shanghai.aliyuncs.com/disk/动手学系列/动手学强化学习/课件pdf/2-马尔可夫决策过程.pdf)

其他参考：[强化学习从基础到进阶-常见问题和面试必知必答[2]](https://zhuanlan.zhihu.com/p/638404534)



**马尔可夫决策过程**（Markov decision process，MDP）是强化学习的重要概念。强化学习中的环境一般就是一个马尔可夫决策过程。与多臂老虎机问题不同，马尔可夫决策过程包含状态信息以及状态之间的转移机制。

### 随机过程

**随机过程**（stochastic process）是概率论的“动力学”部分。概率论的研究对象是*静态*的随机现象，而<u>随机过程的研究对象是随时间演变的随机现象</u>（例如天气随时间的变化、城市交通随时间的变化），<u>研究*动态*随机现象的统计规律</u>。

### 马尔可夫性质

**马尔可夫性质**（Markov property）用于描述状态转移，即下一个状态只与当前状态和决策有关，与之前的状态无关。换句话说，马尔科夫性质表示在给定当前状态的情况下，过去的状态对于未来的状态没有影响。

- 状态从历史（history）中捕获了所有相关信息
- 当状态已知的时候，可以抛开历史不管
- 也就是说，当前状态是未来的充分统计量

### 马尔可夫过程

**马尔可夫过程**（Markov process）指具有马尔可夫性质的随机过程，也被称为**马尔可夫链**（Markov chain）。

**马尔可夫链（Markov chain）**： 概率论和数理统计中具有马尔可夫性质且存在于离散的指数集（index set）和状态空间（state space）内的随机过程（stochastic process）。

**马尔可夫奖励过程（Markov reward process，MRP）**： 本质是马尔可夫链加上一个奖励函数。在马尔可夫奖励过程中，状态转移矩阵和它的状态都与马尔可夫链的一样，只多了一个奖励函数。奖励函数是一个期望，即在某一个状态可以获得多大的奖励。

我们通常用元组<S,P>描述一个马尔可夫过程。P是**状态转移矩阵**（state transition matrix）。状态转移矩阵类似于条件概率（conditional probability），其表示当智能体到达某状态后，到达其他所有状态的概率。状态转移矩阵定义了所有状态对之间的转移概率。（其他参数参见[MDP五元组](#MDP五元组)）

### 马尔可夫决策过程

马尔可夫决策过程（Markov Decision Process，MDP）提供了一套为在结果部分随机、部分在决策者的控制下的决策过程建模的数学框架。

在MDP中，通常存在一个智能体来执行动作。智能体的目标是最大化得到的累计奖励。智能体根据当前状态从动作的集合中选择一个动作的函数，被称为策略。

MDP形式化地描述了一种强化学习的环境：环境完全可观测。即，当前状态可以完全表征过程（[马尔可夫性质](#马尔可夫性质)）。

#### 策略

智能体的**策略**（Policy）通常用字母$\pi$表示。 $\pi(s): S \rightarrow A $

**确定性策略**（deterministic policy）中，它在每个状态时只输出一个确定性的动作。

**随机性策略**（stochastic policy）中，它在每个状态时输出的是关于动作的概率分布，然后根据该分布进行采样就可以得到一个动作。

#### 状态价值函数

我们用$V^\pi(s)$表示在 MDP 中基于策略的状态价值函数（state-value function），定义为从状态s出发遵循策略$\pi$能获得的期望回报。数学表达式见此处 <a href="#价值函数">跳转</a>

#### 动作价值函数

不同于 MRP，在 MDP 中，由于动作的存在，我们额外定义一个(状态）**动作价值函数**（action-value function）。我们用$Q^\pi(s,a)$表示在 MDP 遵循策略时，对当前状态s执行动作a得到的期望回报，按照某一策略$\pi$与环境继续进行交互，得到的累计汇报的期望值。

![image-20231008203916299](/images/image-20231008203916299.png)

#### 贝尔曼期望方程

在贝尔曼方程中加上“期望”二字是为了与接下来的贝尔曼最优方程进行区分。我们通过简单推导就可以分别得到两个价值函数的**贝尔曼期望方程**（Bellman Expectation Equation）：

![image-20231008204157071](/images/image-20231008204157071.png)

(以上2式的推导参见[动作价值函数](#动作价值函数))

### MDP五元组

 MDP可以由一个五元组表示 $(S, 𝐴, 𝑃_{𝑠𝑎} , 𝛾, 𝑅)$

- 𝑆是状态的集合
  比如，迷宫中的位置，Atari游戏中的当前屏幕显示
- 𝐴是动作的集合
  比如，向N、E、S、W移动，手柄操纵杆方向和按钮
- $P_{sa}$是状态转移概率
  对每个状态𝑠 ∈ 𝑆和动作𝑎 ∈ 𝐴，P𝑠𝑎是下一个状态在S中的概率分布
- 𝛾 ∈ [0,1]是对未来奖励的折扣因子
  引入折扣因子的理由为远期利益具有一定不确定性，有时我们更希望能够尽快获得一些奖励，所以我们需要对远期利益打一些折扣。接近 1 的𝛾更关注长期的累计奖励，接近 0 的𝛾更考虑短期奖励。
- 𝑅: 𝑆 × 𝐴 ⟼ ℝ 是奖励函数
  有时奖励只和状态相关

#### 为什么在马尔可夫奖励过程中需要有折扣因子？[^强化学习从基础到进阶-常见问题和面试必知必答2]

1. 
   首先，是有些马尔可夫过程是环状的，它并没有终点，所以我们想避免无穷的奖励。

2. 另外，我们想把不确定性也表示出来，希望尽可能快地得到奖励，而不是在未来的某个时刻得到奖励。

3. 接上一点，如果这个奖励是有实际价值的，我们可能更希望立刻就得到奖励，而不是后面才可以得到奖励。

4. 还有，在有些时候，折扣因子也可以设为0。当它被设为0后，我们就只关注它当前的奖励。我们也可以把它设为1，设为1表示未来获得的奖励与当前获得的奖励是一样的。


所以，折扣因子可以作为强化学习智能体的一个超参数进行调整，然后就会得到不同行为的智能体。

### MDP的动态

MDP的动态如下所示：

1. 从状态$s_0$开始
2. 智能体选择某个动作$a_0 \in A$
3. 智能体得到奖励$R(s_0, a_0)$
4. MDP随机转移到下一个状态$s_1 ~P_{s_0a_0}$

这个过程不断进行，直到<font color=DodgerBlue>终止状态$s_𝑇$出现为止</font>，或者<font color=DodgerBlue>无止尽地进行下去</font>
$$
s_{0} \stackrel{a_{0}, R\left(s_{0}, a_{0}\right)}{\longrightarrow} s_{1} \stackrel{a_{1}, R\left(s_{1}, a_{1}\right)}{\longrightarrow} s_{2} \stackrel{a_{2}, R\left(s_{2}, a_{2}\right)}{\longrightarrow} s_{3} \cdots
$$
所有奖励的衰减之和称为**回报**$G_t$（Return），智能体的总回报为
$$
R\left(s_{0}, a_{0}\right)+\gamma R\left(s_{1}, a_{1}\right)+\gamma^{2} R\left(s_{2}, a_{2}\right)+\cdots
$$
在大部分情况下，奖励只和状态相关。比如，在迷宫游戏中，奖励只和位置相关；在围棋中，奖励只基于最终所围地盘的大小有关。所以上述过程和奖励函数略有修改。

奖励函数为$R(s): S \mapsto \mathbb{R}$

MDP的过程为
$$
s_{0} \stackrel{a_{0}, R\left(s_{0}\right)}{\longrightarrow} s_{1} \stackrel{a_{1}, R\left(s_{1}\right)}{\longrightarrow} s_{2} \stackrel{a_{2}, R\left(s_{2}\right)}{\longrightarrow} s_{3} \cdots
$$
累积奖励为
$$
R\left(s_{0}\right)+\gamma R\left(s_{1}\right)+\gamma^{2} R\left(s_{2}\right)+\cdots
$$

### MDP目标和策略

MDP的目标：选择能够最大化累积奖励期望的动作

$$
\mathbb{E}\left[R\left(s_{0}\right)+\gamma R\left(s_{1}\right)+\gamma^{2} R\left(s_{2}\right)+\cdots\right]
$$
上述该式子中，由于状态(环境)s本身是stochastic，Policy也可能是stochastic，reward也是stochastic。为了最后去优化的东西是一个确定的标量，所有前面加上了Expectation，即$\mathbb{E}$。

给定一个特定的策略$ \pi(s): S \rightarrow A$。在状态s下采取动作有$a=\pi(s)$。

给策略$ \pi $定义<a name="价值函数">价值函数</a>

![image-20231008132133758](/images/image-20231008132133758.png)

当前的立即奖励  加上  时间折扣𝛾  乘  接下来状态可能会转移到下个s‘的状态(由状态转移的概率分布已经决定了的)  乘上  价值函数。

上式就是马尔可夫奖励过程中非常有名的**贝尔曼方程**（Bellman equation）。通过上式，我们就可以去不断地通过迭代的方式去更新我们对于当前这个$V^\pi$的认知，最后达到收敛获得真正的这样一个价值函数的值。

### 马尔可夫奖励过程与马尔可夫决策过程的区别是什么？[^强化学习从基础到进阶-常见问题和面试必知必答2]

马尔可夫过程和马尔可夫奖励过程都是*自发改变的随机过程*；而如果<u>有一个外界的“刺激”来共同改变</u>这个随机过程，就有了**[马尔可夫决策过程](#马尔可夫决策过程)**（Markov decision process，MDP）。我们将这个来自外界的刺激称为**智能体**（agent）的动作，在马尔可夫奖励过程（MRP）的基础上加入动作，就得到了马尔可夫决策过程（MDP）。

相对于马尔可夫奖励过程，马尔可夫决策过程多了一个决策过程，通常存在一个智能体来执行动作。由于多了一个决策，多了一个动作，因此状态转移也多了一个自变量a，即执行一个动作，导致未来状态的变化，其不仅依赖于当前的状态，也依赖于在当前状态下智能体采取的动作决定的状态变化。对于价值函数，它也多了一个条件，多了一个当前的动作，即当前状态以及采取的动作会决定当前可能得到的奖励的多少。

另外，两者之间是有转换关系的。具体来说，已知一个马尔可夫决策过程以及一个策略 $\pi$ 时，我们可以把马尔可夫决策过程转换成马尔可夫奖励过程。

MDR中，不再使用类似 MRP 定义中的状态转移矩阵方式，而是直接表示成了状态转移函数。这样做一是因为此时状态转移与动作a也有关，变成了一个三维数组，而不再是一个矩阵（二维数组）；二是因为状态转移函数更具有一般意义，连续状态的 MDP 环境仍然可以用状态转移函数表示。

在马尔可夫决策过程中，状态的转移函数 $P(s'|s,a)$ 是基于它的当前状态和当前动作的，因为我们现在已知策略函数，即在每一个状态，我们知道其采取每一个动作的概率，所以我们就可以直接把这个动作进行加和，就可以得到对于马尔可夫奖励过程的一个转移概率。同样地，对于奖励，我们可以把动作去掉，这样就会得到一个类似于马尔可夫奖励过程的奖励。

### 最优价值函数

强化学习的目标通常是找到一个策略，使得智能体从初始状态出发能获得最多的期望回报。搜索一种策略 $\pi$ ，使每个状态的价值最大，$V^*$ 就是到达每一个状态的极大值。在极大值中，我们得到的策略是最佳策略。最佳策略使得每个状态的价值函数都取得最大值。

公式见下一节。

### 占用度量



学习中，待补充

<span style="background:yellow">文本</span>

---

Ref:

1. [^动手学强化学习]: https://hrl.boyuai.com/chapter/1/时序差分算法/

2. [^Chatgpt]: https://chat.openai.com/

3. [^强化学习从基础到进阶-常见问题和面试必知必答2]: https://zhuanlan.zhihu.com/p/638404534