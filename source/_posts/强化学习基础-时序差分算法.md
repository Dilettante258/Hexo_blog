---
title: 强化学习基础-时序差分算法
categories: 强化学习
date: 2023-10-7 20:34:00
mathjax: true
---

书接上一节【动态规划算法】.

动态规划算法适用于已知马尔可夫决策过程的情况，可以直接解出最优价值或策略。但在大部分情况下，马尔可夫决策过程的状态转移概率是未知的，这时就需要使用无模型的强化学习算法。无模型的强化学习算法不需要事先知道环境的奖励函数和状态转移函数，而是通过与环境交互采样数据来学习。

模型无关的强化学习直接从经验中学习值（value）和策略 （policy），而无需构建马尔可夫决策过程模型（MDP）。关键步骤：（1）估计值函数；（2）优化策略

无模型的强化学习算法基于时序差分（temporal difference，TD）学习，其中两个经典算法是Sarsa和Q-learning。Sarsa算法是一种在线策略学习算法，它使用当前策略下采样得到的样本进行学习。Q-learning算法是一种离线策略学习算法，它使用经验回放池将之前采样得到的样本收集起来再次利用。

离线策略学习使用历史数据进行学习，可以更好地利用历史数据，并具有更小的样本复杂度。因此，离线策略学习在实际应用中更为广泛。

## 值函数估计

在基于模型的强化学习（MDP）中，值函数能够通过动态规划计算获得
$$
\begin{aligned}
V^{\pi}(s)& =\mathbb{E}[R(s_0)+\gamma R(s_1)+\gamma^2R(s_2)+\cdots|s_0=s,\pi]  \\
&=R(s)+\gamma\sum_{s^{\prime}\in\mathcal{S}}P_{s\pi(s)}(s^{\prime})V^{\pi}(s^{\prime})
\end{aligned}
$$
 在模型无关的强化学习中，我们无法直接获得 $𝑃_{𝑠𝑎}$ 和 𝑅 ，但是，我们拥有一系列可以用来估计值函数的经验或者说Episodes。

### 蒙特卡洛方法

蒙特卡洛方法（Monte-Carlo methods）是一类广泛的计算算法，依赖于重复随机抽样来获得数值结果。生活中处处都是MC方法。 
例如，计算圆的面积，围棋对弈中估计当前状态下的胜率。

### 蒙特卡洛价值估计

回顾：

- 累计奖励（return）是总折扣奖励
  $$
  G_t=R_{t+1}+\gamma R_{t+2}+\cdots\gamma^{T-1}R_T
  $$

- 值函数（value function）是期望累计奖励

  $$
  \begin{aligned}
  V^{\pi}(s)& =\mathbb{E}[R(s_0)+\gamma   R(s_1)+\gamma^2R(s_2)+\cdots|s_0=s,\pi]  \\
  &=\mathbb{E}[G_t|s_t=s,\pi] \\
  &\simeq\frac1N\sum_{i=1}^NG_t^{(i)}
  \end{aligned}
  $$

  上式使用策略𝜋从状态𝑠采样𝑁个片段，从黑盒中 计算平均累计奖励。

蒙特卡洛策略评估使用<font color=DodgerBlue>经验均值累计奖励</font>而不是<font color=DodgerBlue>期望累计奖励</font>。

目标：从策略 𝜋下的经验片段学习 $𝑉^𝜋$
$$
S_0^{(i)}\underset{R_1^{(i)}}{\operatorname*{\overset{a_0^{(i)}}{\operatorname*{\longrightarrow}}}}S_1^{(i)}\underset{R_2^{(i)}}{\operatorname*{\overset{a_1^{(i)}}{\operatorname*{\longrightarrow}}}}S_2^{(i)}\underset{R_3^{(i)}}{\operatorname*{\overset{a_2^{(i)}}{\operatorname*{\longrightarrow}}}}S_3^{(i)}...S_T^{(i)}\boldsymbol{\sim}\pi
$$
在一个片段中的<font color=DodgerBlue>每个时间步长𝑡</font>的<font color=DodgerBlue>状态𝑠</font>都被访问

- 增量计数器 𝑁(𝑠) ← 𝑁(𝑠) + 1

- 增量总累计奖励 𝑆(𝑠) ← 𝑆(𝑠) + 𝐺𝑡

- 价值被估计为累计奖励的均值 𝑉(𝑠) = 𝑆(𝑠)/𝑁(𝑠)

- 由大数定率有
  $$
  V(s)\to V^{\pi}(s)\mathrm{~as~}N(s)\to\infty
  $$
  

增量蒙特卡洛更新

- 每个片段结束后逐步更新𝑉(𝑠) 
- 对于每个状态$𝑆_𝑡$和对应累计奖励$𝐺_t$

$$
\begin{aligned}&N(S_t)\leftarrow N(S_t)+1\\&V(S_t)\leftarrow V(S_t)+\frac1{N(S_t)}\big(G_t-V(S_t)\big)\end{aligned}
$$

- 对于非稳定的问题（即，环境会随时间发生变化），我们可以跟踪一个现阶段的平均值（即，不考虑过久之前的片段）
  $$
  V(S_t)\leftarrow V(S_t)+\alpha(G_t-V(S_t))
  $$
  上式可化为 $ (1- α )V(S_t) + αG_t $



**小结**
$$
\begin{aligned}\text{思路:}\quad&V(S_t)\simeq\frac1N\sum_{i=1}^NG_t^{(i)}\\ \text{实现:}\quad&V(S_t)\leftarrow V(S_t)+\alpha\big(G_t-V(S_t)\big)\end{aligned}
$$

- 蒙特卡洛方法：直接从经验片段进行学习
- 蒙特卡洛是模型无关的：未知马尔可夫决策过程的状态转移/奖励，直接通过采样的方式去逼近，通过大数定理的方式去逼近这样一个期望值。
- 蒙特卡洛必须从完整的片段中进行学习：没有使用bootstrapping的方法(居于碎片自举法)
- 蒙特卡洛采用最简单的思想：值（value）= 平均累计奖励（mean  return）
- 注意：只能将蒙特卡洛方法应用于有限长度的马尔可夫决策过程中。即，<u>所有的片段都有终止状态</u>。不然只能用时序差分算法。



## 重要性采样

重要性采样是在机器学习当中的一个非常重要的概念，因为在机器学习当中我们往往需要去优化的是在一定数据分布上的一个损失函数。而如果我们拿到的训练数据，他是在另外一个数据分布下面的话，我们就可以考虑重要性采样。
我们首先通过数学的方法来分析一下重要性采样它的本质原理。

估计一个不同分布的期望：
$$
\begin{aligned}
\mathbb{E}_{x\sim p}[f(x)]& =\int_xp(x)f(x)dx  \\
&=\int_xq(x)\frac{p(x)}{q(x)}f(x)dx \\
&=\mathbb{E}_{x\sim q}\left[\frac{p(x)}{q(x)}f(x)\right]
\end{aligned}
$$
将每个实例的权重重新分配为$\beta(x)=\frac{p(x)}{q(x)}$



<span style="background:yellow">待补充待补充待补充</span>

## 时序差分学习（Temporal Difference Learning）

无模型学习中需要找到一个当前目标，学习的目标主要是当前状态的真正地V(s)是多少。要估计它，可以


$$
G_t=R_{t+1}+\gamma R_{t+2}+\gamma^2R_{t+3}+\cdots=R_{t+1}+\gamma V(S_{t+1})\\V(S_t)\leftarrow V(S_t)+\alpha(R_{t+1}+\gamma V(S_{t+1})-V(S_t))
$$
$R_{t+1}$:观测值		$V(S_{t+1})$:对未来的猜测



时序差分方法直接<font color=deepskyblue>从经验片段中进行学习</font>

时序差分是<font color=coral>模型无关</font>的：不需要预先获取马尔可夫决策过程的状态转移/奖励

通过bootstrapping，时序差分从<font color=deepskyblue>不完整的片段中学习</font>

时序差分更新当前预测值使之接近估计累计奖励（观测值+对未来的猜测，<font color=coral>非真实值</font>）

## 蒙特卡洛 vs. 时序差分（MC vs. TD)

相同的目标：<font color=coral>从策略𝜋下的经验片段学习$𝑉^𝜋 $</font>

- 增量地进行每次蒙特卡洛过程（<font color=coral>MC</font>）

  - 更新值函数$𝑉(𝑆_𝑡)$使之接近<font color=deepskyblue>准确累计奖励$𝐺_t$</font>


$$
  V(S_t)\leftarrow V(S_t)+\alpha\big(G_t-V(S_t)\big)
$$

- 最简单的时序差分学习算法（TD）：

  - 更新$V(S_t)$使之接近估计累计奖励$R_{t+1}+\gamma V(S_{t+1})$


$$
  V(S_t)\leftarrow V(S_t)+\alpha(R_{t+1}+\gamma V(S_{t+1})-V(S_t))
$$

  - 时序差分目标：$R_{t+1}+\gamma V(S_{t+1})$

  - 时序差分误差：$\delta_{t}=R_{t+1}+\gamma V(S_{t+1})-V(S_{t})$

## 偏差（Bias）/方差（Variance）的权衡

两者目标相同：从策略 𝜋下的经验片段学习(估计) $𝑉^𝜋$

偏差：

1. MC增量地更新$V(S_t)$逼近准确累计奖励$G_t$			无偏估计
2. TD更新$V(S_t)$逼近估计累计奖励$R_{t+1}+\gamma V^{\pi}(S_{t+1})$	有偏估计（观测值+对未来的猜测）

> MC中累计奖励 $G_t=R_{t+1}+\gamma R_{t+2}+\cdots+\gamma^{T-1}R_T$ 是 $V^{\pi}(S_t)$ 的无偏估计	$\mathbb{E}\left[G_t \right]=V^{\pi}(s_t)$
>
> 时序差分<font color=deepskyblue>真实目标 </font>$R_{t+1}+\gamma V^{\pi}(S_{t+1})$是$V^{\pi}(S_t)$的无偏估计
>
> 时序差分<font color=deepskyblue>目标</font><a name="时序差分目标"> </a> $R_{t+1}+\gamma V(S_{t+1})$是$V^{\pi}(S_t)$的有偏估计（观测值+对未来的猜测）

> **无偏估计：** 无偏估计是指在进行参数估计时，估计值的期望与真实参数值相等。
> **有偏估计：** 有偏估计是指在进行参数估计时，估计值的期望与真实参数值不相等。换句话说，有偏估计会系统地高估或低估参数的真实值。

时序差分目标有比累计奖励更低的方差，更大的偏差

- MC累计奖励——取决于多步随机动作，多步状态转移和多步奖励
- 时序差分目标——取决于单步随机动作，单步状态转移和单步奖励

$G_t$每一步的采样都有不确定性，因此不同采样的$G_t$可能方差很大。TD相比之下不确定性来自于$R_{t+1}$的采样，学习更加稳定。
TD<a href="#时序差分目标">目标</a>是有偏的，而MC是无偏的。

## 蒙特卡洛(MC)和时序差分(TD) 的优缺点

- 时序差分:能够在知道最后结果之前进行学习

  - 时序差分能够在<font color=deepskyblue>每一步之后进行在线学习</font>
  - 蒙特卡洛<font color=deepskyblue>必须等待片段结束</font>，直到累计奖励已知

  

- 时序差分:能够无需最后结果地进行学习

  - 时序差分能够从不完整的序列中学习
  - 蒙特卡洛只能从完整序列中学习
  - 时序差分在连续（<font color=coral>无终止的</font>）环境下工作
  - 蒙特卡洛只能在片段化的 （<font color=coral>有终止的</font>）环境下工作



<img src="/images/image-20231011221659410.png" alt="image-20231011221659410" style="zoom: 80%;" />

## 多步时序查分学习

对于有时间约束的情况，我们可以跳过𝑛步预测的部分，直接进 入模型无关的控制

定义𝑛步累计奖励
$$
G_t^{(n)}=R_{t+1}+\gamma R_{t+2}+\cdots+\gamma^{n-1}R_{t+n}+\gamma^nV(S_{t+n})
$$
𝑛步时序差分学习
$$
V(S_t)\leftarrow V(S_t)+\alpha\left(G_t^{(n)}-V(S_t)\right)
$$


