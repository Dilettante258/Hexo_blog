---
title: 强化学习基础-时序差分算法
categories: 强化学习
date: 2023-10-11 20:34:00
mathjax: true
---

书接上一节[【动态规划算法】](https://www.dilettante258.cyou/2023/10/强化学习基础-动态规划算法/)

原链接：[动手学强化学习](https://hrl.boyuai.com/chapter/1/时序差分算法)

PDF课件：[值函数估计](https://boyuai.oss-cn-shanghai.aliyuncs.com/disk/动手学系列/动手学强化学习/课件pdf/3-值函数估计.pdf)	[无模型控制](https://boyuai.oss-cn-shanghai.aliyuncs.com/disk/动手学系列/动手学强化学习/课件pdf/4-无模型控制.pdf)

---

动态规划算法适用于已知马尔可夫决策过程的情况，可以直接解出最优价值或策略。但在大部分情况下，马尔可夫决策过程的状态转移概率是未知的，这时就需要使用无模型的强化学习算法。无模型的强化学习算法不需要事先知道环境的奖励函数和状态转移函数，而是通过与环境交互采样数据来学习。

模型无关的强化学习直接从经验中学习值（value）和策略 （policy），而无需构建马尔可夫决策过程模型（MDP）。关键步骤：（1）估计值函数；（2）优化策略

无模型的强化学习算法基于时序差分（temporal difference，TD）学习，其中两个经典算法是Sarsa和Q-learning。Sarsa算法是一种在线策略学习算法，它使用当前策略下采样得到的样本进行学习。Q-learning算法是一种离线策略学习算法，它使用经验回放池将之前采样得到的样本收集起来再次利用。

离线策略学习使用历史数据进行学习，可以更好地利用历史数据，并具有更小的样本复杂度。因此，离线策略学习在实际应用中更为广泛。

## 值函数估计

在基于模型的强化学习（MDP）中，值函数能够通过动态规划计算获得
$$
\begin{aligned}
V^{\pi}(s)& =\mathbb{E}[R(s_0)+\gamma R(s_1)+\gamma^2R(s_2)+\cdots|s_0=s,\pi]  \\
&=R(s)+\gamma\sum_{s^{\prime}\in\mathcal{S}}P_{s\pi(s)}(s^{\prime})V^{\pi}(s^{\prime})
\end{aligned}
$$
 在模型无关的强化学习中，我们无法直接获得$𝑃_{𝑠𝑎}$和 𝑅 ，但是，我们拥有一系列可以用来估计值函数的经验或者说Episodes。

### 蒙特卡洛方法

蒙特卡洛方法（Monte-Carlo methods）是一类广泛的计算算法，依赖于重复随机抽样来获得数值结果。生活中处处都是MC方法。 
例如，计算圆的面积，围棋对弈中估计当前状态下的胜率。

### 蒙特卡洛价值估计

回顾：

- 累计奖励（return）是总折扣奖励
$$
  G_t=R_{t+1}+\gamma R_{t+2}+\cdots\gamma^{T-1}R_T
$$

- 值函数（value function）是期望累计奖励

$$
  \begin{aligned}
  V^{\pi}(s)& =\mathbb{E}[R(s_0)+\gamma   R(s_1)+\gamma^2R(s_2)+\cdots|s_0=s,\pi]  \\
  &=\mathbb{E}[G_t|s_t=s,\pi] \\
  &\simeq\frac1N\sum_{i=1}^NG_t^{(i)}
  \end{aligned}
$$

  上式使用策略𝜋从状态𝑠采样𝑁个片段，从黑盒中 计算平均累计奖励。

蒙特卡洛策略评估使用<font color=DodgerBlue>经验均值累计奖励</font>而不是<font color=DodgerBlue>期望累计奖励</font>。

目标：从策略 𝜋下的经验片段学习$𝑉^𝜋$
$$
S_0^{(i)}\underset{R_1^{(i)}}{\operatorname*{\overset{a_0^{(i)}}{\operatorname*{\longrightarrow}}}}S_1^{(i)}\underset{R_2^{(i)}}{\operatorname*{\overset{a_1^{(i)}}{\operatorname*{\longrightarrow}}}}S_2^{(i)}\underset{R_3^{(i)}}{\operatorname*{\overset{a_2^{(i)}}{\operatorname*{\longrightarrow}}}}S_3^{(i)}...S_T^{(i)}\boldsymbol{\sim}\pi
$$
在一个片段中的<font color=DodgerBlue>每个时间步长𝑡</font>的<font color=DodgerBlue>状态𝑠</font>都被访问

- 增量计数器 𝑁(𝑠) ← 𝑁(𝑠) + 1

- 增量总累计奖励 𝑆(𝑠) ← 𝑆(𝑠) + 𝐺𝑡

- 价值被估计为累计奖励的均值 𝑉(𝑠) = 𝑆(𝑠)/𝑁(𝑠)

- 由大数定率有
$$
  V(s)\to V^{\pi}(s)\mathrm{~as~}N(s)\to\infty
$$


增量蒙特卡洛更新

- 每个片段结束后逐步更新𝑉(𝑠) 
- 对于每个状态$𝑆_𝑡$和对应累计奖励$𝐺_t$

$$
\begin{aligned}&N(S_t)\leftarrow N(S_t)+1\\&V(S_t)\leftarrow V(S_t)+\frac1{N(S_t)}\big(G_t-V(S_t)\big)\end{aligned}
$$

- 对于非稳定的问题（即，环境会随时间发生变化），我们可以跟踪一个现阶段的平均值（即，不考虑过久之前的片段）
$$
  V(S_t)\leftarrow V(S_t)+\alpha(G_t-V(S_t))
$$
  上式可化为$(1- α )V(S_t) + αG_t$



**小结**
$$
\begin{aligned}\text{思路:}\quad&V(S_t)\simeq\frac1N\sum_{i=1}^NG_t^{(i)}\\ \text{实现:}\quad&V(S_t)\leftarrow V(S_t)+\alpha\big(G_t-V(S_t)\big)\end{aligned}
$$

- 蒙特卡洛方法：直接从经验片段进行学习
- 蒙特卡洛是模型无关的：未知马尔可夫决策过程的状态转移/奖励，直接通过采样的方式去逼近，通过大数定理的方式去逼近这样一个期望值。
- 蒙特卡洛必须从完整的片段中进行学习：没有使用bootstrapping的方法(居于碎片自举法)
- 蒙特卡洛采用最简单的思想：值（value）= 平均累计奖励（mean  return）
- 注意：只能将蒙特卡洛方法应用于有限长度的马尔可夫决策过程中。即，<u>所有的片段都有终止状态</u>。不然只能用时序差分算法。



## 重要性采样

重要性采样是在机器学习当中的一个非常重要的概念，因为在机器学习当中我们往往需要去优化的是在一定数据分布上的一个损失函数。而如果我们拿到的训练数据，他是在另外一个数据分布下面的话，我们就可以考虑重要性采样。
我们首先通过数学的方法来分析一下重要性采样它的本质原理。

估计一个不同分布的期望：
$$
\begin{aligned}
\mathbb{E}_{x\sim p}[f(x)]& =\int_xp(x)f(x)dx  \\
&=\int_xq(x)\frac{p(x)}{q(x)}f(x)dx \\
&=\mathbb{E}_{x\sim q}\left[\frac{p(x)}{q(x)}f(x)\right]
\end{aligned}
$$
将每个实例的权重重新分配为$\beta(x)=\frac{p(x)}{q(x)}$



<span style="background:yellow">待补充待补充待补充</span>

## 时序差分学习（Temporal Difference Learning）

无模型学习中需要找到一个当前目标，学习的目标主要是当前状态的真正地V(s)是多少。要估计它，可以


$$
G_t=R_{t+1}+\gamma R_{t+2}+\gamma^2R_{t+3}+\cdots=R_{t+1}+\gamma V(S_{t+1})\\V(S_t)\leftarrow V(S_t)+\alpha(R_{t+1}+\gamma V(S_{t+1})-V(S_t))
$$
$R_{t+1}$:观测值		$V(S_{t+1})$:对未来的猜测



时序差分方法直接<font color=deepskyblue>从经验片段中进行学习</font>

时序差分是<font color=coral>模型无关</font>的：不需要预先获取马尔可夫决策过程的状态转移/奖励

通过bootstrapping，时序差分从<font color=deepskyblue>不完整的片段中学习</font>

时序差分更新当前预测值使之接近估计累计奖励（观测值+对未来的猜测，<font color=coral>非真实值</font>）

## 蒙特卡洛 vs. 时序差分（MC vs. TD)

相同的目标：<font color=coral>从策略𝜋下的经验片段学习$𝑉^𝜋$</font>

- 增量地进行每次蒙特卡洛过程（<font color=coral>MC</font>）

  - 更新值函数$𝑉(𝑆_𝑡)$使之接近<font color=deepskyblue>准确累计奖励$𝐺_t$</font>


$$
  V(S_t)\leftarrow V(S_t)+\alpha\big(G_t-V(S_t)\big)
$$

- 最简单的时序差分学习算法（TD）：

  - 更新$V(S_t)$使之接近估计累计奖励$R_{t+1}+\gamma V(S_{t+1})$


$$
  V(S_t)\leftarrow V(S_t)+\alpha(R_{t+1}+\gamma V(S_{t+1})-V(S_t))
$$

  - 时序差分目标：$R_{t+1}+\gamma V(S_{t+1})$

  - 时序差分误差：$\delta_{t}=R_{t+1}+\gamma V(S_{t+1})-V(S_{t})$

## 偏差（Bias）/方差（Variance）的权衡

两者目标相同：从策略 𝜋下的经验片段学习(估计)$𝑉^𝜋$

偏差：

1. MC增量地更新$V(S_t)$逼近准确累计奖励$G_t$			无偏估计
2. TD更新$V(S_t)$逼近估计累计奖励$R_{t+1}+\gamma V^{\pi}(S_{t+1})$	有偏估计（观测值+对未来的猜测）

> MC中累计奖励$G_t=R_{t+1}+\gamma R_{t+2}+\cdots+\gamma^{T-1}R_T$是$V^{\pi}(S_t)$的无偏估计	$\mathbb{E}\left[G_t \right]=V^{\pi}(s_t)$
>
> 时序差分<font color=deepskyblue>真实目标 </font>$R_{t+1}+\gamma V^{\pi}(S_{t+1})$是$V^{\pi}(S_t)$的无偏估计
>
> 时序差分<font color=deepskyblue>目标</font><a name="时序差分目标"> </a>$R_{t+1}+\gamma V(S_{t+1})$是$V^{\pi}(S_t)$的有偏估计（观测值+对未来的猜测）

> **无偏估计：** 无偏估计是指在进行参数估计时，估计值的期望与真实参数值相等。
> **有偏估计：** 有偏估计是指在进行参数估计时，估计值的期望与真实参数值不相等。换句话说，有偏估计会系统地高估或低估参数的真实值。

时序差分目标有比累计奖励更低的方差，更大的偏差

- MC累计奖励——取决于多步随机动作，多步状态转移和多步奖励
- 时序差分目标——取决于单步随机动作，单步状态转移和单步奖励

$G_t$每一步的采样都有不确定性，因此不同采样的$G_t$可能方差很大。TD相比之下不确定性来自于$R_{t+1}$的采样，学习更加稳定。
TD<a href="#时序差分目标">目标</a>是有偏的，而MC是无偏的。

## 蒙特卡洛(MC)和时序差分(TD) 的优缺点

- 时序差分:能够在知道最后结果之前进行学习

  - 时序差分能够在<font color=deepskyblue>每一步之后进行在线学习</font>
  - 蒙特卡洛<font color=deepskyblue>必须等待片段结束</font>，直到累计奖励已知

  

- 时序差分:能够无需最后结果地进行学习

  - 时序差分能够从不完整的序列中学习
  - 蒙特卡洛只能从完整序列中学习
  - 时序差分在连续（<font color=coral>无终止的</font>）环境下工作
  - 蒙特卡洛只能在片段化的 （<font color=coral>有终止的</font>）环境下工作



<img src="/images/image-20231011221659410.png" alt="image-20231011221659410" style="zoom: 80%;" />

## 多步时序查分学习

对于有时间约束的情况，我们可以跳过𝑛步预测的部分，直接进 入模型无关的控制

定义𝑛步累计奖励
$$
G_t^{(n)}=R_{t+1}+\gamma R_{t+2}+\cdots+\gamma^{n-1}R_{t+n}+\gamma^nV(S_{t+n})
$$
𝑛步时序差分学习
$$
V(S_t)\leftarrow V(S_t)+\alpha\left(G_t^{(n)}-V(S_t)\right)
$$





## SARSA

在当前策略下执行的每个（<font color=coral>状态-动作-奖励-状态-动作</font>）元组
所以的它简称的是(S-A-R-S'-A') 的这样一个5元组，贪婪算法来选取<font color=deepskyblue>*在某个状态下*</font><font color=teal>动作价值</font>最大的那个动作，即$\arg\max_aQ(s,a)$。用贪婪算法根据动作价值选取动作来和环境交互，再根据得到的数据用时序差分算法更新动作价值估计。

![image-20231012212139248](images/image-20231012212139248.png)

 SARSA更新状态-动作值函数为
$$
Q(s,a)\gets Q(s,a)+\alpha(r+\gamma Q(s^{\prime},a^{\prime})-Q(s,a))
$$
这个简单的算法存在两个需要进一步考虑的问题。

- SARSA算法是一种时序差分学习算法，通过不断地更新状态-动作对的价值函数来学习最优策略。在传统的SARSA算法中，为了准确地估计状态价值函数，需要使用大量的样本进行更新。然而，作者指出，实际上我们可以使用较少的样本来评估策略，然后进行策略的更新。这是因为策略提升可以在策略评估尚未完全进行的情况下进行，即可以在部分样本的基础上进行策略的改进。这种思想与广义策略迭代的思想相符合，广义策略迭代是一种通过交替进行策略评估和策略改进的方法来逐步优化策略的过程。因此，作者认为在SARSA算法中，我们可以忽略使用大量样本的要求，而只使用一些样本进行策略评估和更新，从而简化算法的实现。
- 某些状态动作对(s,a)可能永远没有在序列中出现，以至于无法对其动作价值进行估计，进而无法保证策略提升后的策略比之前的好。简单常用的解决方案是不再一味使用贪婪算法，而是采用一个ε-greedy贪婪策略

![image-20231013174604626](/images/image-20231013174604626.png)

初始化$Q(s,a)$

for 序列$e=1\to E$  do:{
	得到初始状态$s$
	用$\epsilon$-greedy 策略根据$Q$选择当前状态$s$下的动作$a$
 	for 时间步$t=1\to T$ do :{
		得到环境反馈的$r,s^{\prime}$
		用$\epsilon$-greedy 策略根据$Q$选择当前状态$s^{\prime}$下的动作$a^{\prime}$
		$Q(s,a)\gets Q(s,a)+\alpha[r+\gamma Q(s^{\prime},a^{\prime})-Q(s,a)]$
		$s\gets s^{\prime},a\gets a^{\prime}$}
		end for
end for	

在线策略时序差分控制（on-policy TD control）使用当前策略进行动作采样。即，SARSA算法中的两个“A”都是由当前策略选择的

## Q-learning 算法

Q-Learning 是一种无模型（model-free）的强化学习算法，它不依赖于环境的完整知识。我们通过直接从与环境的交互中学习最优策略，不需要计算期望的期间考虑到每个动作的可能概率。是一种离线策略（off-policy）[^非Offline]学习方法， 学习 <u>状态-动作值函数 𝑄(𝑠, 𝑎) ∈ ℝ</u>，不直接优化策略本身。

> Off-policy学习方法允许智能体在学习过程中使用与其当前策略不一致的数据来评估和改进策略。
>
> off-policy学习方法使用两个不同的策略：目标策略和行为策略。目标策略是智能体希望学习的最优策略，而行为策略是智能体在与环境交互时实际采取的策略。智能体通过从行为策略中收集的经验数据来评估和改进目标策略。

<img src=/images/image-20231013181954191.png" alt="image-20231013181954191" style="zoom:50%;" />

### 离线策略学习

什么是离线策略学习

- 目标策略 $\pi(a|s)$ 进行值函数评估	$(V^{\pi}(s)$或$Q^\pi(s,a))$ 
- 行为策略 $\mu(a|s)$ 收集数据		$\quad\{s_1,a_1,r_2,s_2,a_2,...,S_T\}{\sim}\mu$

为什么使用离线策略学习[^来源Chatgpt]

- 数据重用：离线策略学习可以利用已有的数据集进行学习，而不需要在每次策略改进时重新采集数据。这样可以节省时间和资源，并且可以更高效地进行策略评估和改进。
- 策略评估和改进的分离：离线策略学习将策略评估和策略改进解耦。智能体可以根据已有的经验数据来评估不同的策略，而不需要实际执行这些策略。这使得策略评估和改进的过程更加灵活和独立。
- 探索性策略：离线策略学习可以使用探索性策略来更有效地探索环境，平衡探索（exploration）和利用（exploitation）。探索性策略可以是一种随机策略或者是基于先前经验的策略。通过使用探索性策略，离线学习可以更好地发现和利用环境中的潜在奖励信号，从而提高学习效果。
- 策略偏移：离线策略学习可以处理策略偏移的问题。策略偏移指的是目标策略和行为策略之间的差异。通过使用重要性采样等技术，离线学习可以校正由于策略偏移引起的估计偏差，从而更准确地评估和改进目标策略。

### Q 学习

#### 无需重要性采样（为什么？）

- **单步更新**：Q-Learning 使用获得的即时奖励和下一个状态的最大Q值来进行更新，这个过程仅依赖于单步的数据。它不要求整个轨迹的数据来进行更新。
- **偏差-方差权衡**：在强化学习中，我们经常面临偏差-方差权衡的问题。通过重要性采样，我们可以纠正由于使用非行为策略来评估其他策略所导致的偏差。但在Q-Learning中，我们不关心评估一个策略，我们关心的是找到最优策略。所以，我们允许估计值带有一些偏差，只要我们最终能找到一个好的策略。【s,a,r,s‘是当前更新的策略是确定的，可认为分布无关】
- **Off-policy 学习**：Q-Learning 是一种 off-policy 算法，意味着它能从一个策略生成的数据中学习另一个策略。我们使用贪婪策略（或者 *ϵ*-贪婪策略）来选择学习过程中的行动，而不是生成数据的策略。由于Q-Learning直接取下一状态的最大Q值作为估计，它自然地抵消了策略的影响，无需通过重要性采样来纠正。
- **无需显式计算期望**：Q-Learning 更新规则使用了一个实际的奖励值和下一个状态的最大Q值来进行更新，并不需要显式地计算期望值，因此不需要关心在计算期望时采样的重要性。[^Chatgpt]

#### 允许目标策略和行为策略都进行改进

行为策略根据$\epsilon$-greedy，选择动作$a_t{\sim}\mu(\cdot|s_t)$

根据目标策略选择后续动作$a^{\prime}{}_{t+1}{\sim}\pi(\cdot|s_t)$[基于Q导出的Policy，$\pi(s_{t+1})=\arg\max_{a′}Q(s_{t+1},a′)$]，使用贪心策略，不用探索。

$$
\begin{aligned}
\begin{aligned}$Q^{\star}(s_t,a_t)=r_{t+1}+\gamma Q(s_{t+1},a_{t+1}^{\prime})\end{aligned}& =r_{t+1}+\gamma Q(s_{t+1},\arg\max_{a_{t+1}^{\prime}}Q(s_{t+1},a_{t+1}^{\prime}))  \\
&=r_{t+1}+\gamma\max_{t+1}Q(s_{t+1},a_{t+1}^\prime)
\end{aligned}
$$
更新$𝑄(𝑠_𝑡 , 𝑎_𝑡)$的值以逼近目标状态-动作值
$$
Q(s_t,a_t)\leftarrow Q(s_t,a_t)+\alpha(r_{t+1}+\gamma Q(s_{t+1},a_{t+1}^{\prime})-Q(s_t,a_t))
$$

## SARSA和Q-Learning的不同

1. 更新规则：
   - SARSA：SARSA使用一个状态-动作-状态-动作-奖励序列来更新值函数。它通过选择下一步的动作（Action）并观察下一状态（State）来更新当前状态-动作（State-Action）对的值。
   - Q-learning：Q-learning则使用状态-动作-奖励-下一状态来更新值函数。它会选择下一步的动作，但不会观察下一状态，而是假设在下一状态中选择了最佳的动作。
2. 收敛性质：
   - SARSA：SARSA是一个on-policy算法，它会根据当前策略选择下一步的动作，并且会尊重当前策略。因此，它通常会收敛到一个与策略相关的值函数。
   - Q-learning：Q-learning是一个off-policy算法，它会选择下一步的动作，不一定要遵循当前策略。因此，它通常会收敛到一个最优值函数，而不受当前策略的影响。
3. 探索策略：
   - SARSA：SARSA通常使用ε-greedy策略来进行探索，即在大部分情况下选择当前最佳的动作，但有一定概率随机选择其他动作来探索。
   - Q-learning：Q-learning也可以使用ε-greedy策略，但由于它是一个off-policy算法，因此它通常可以更灵活地选择探索动作，不受当前策略的限制。
4. 稳定性：
   - SARSA：由于SARSA是一个on-policy算法，它在某些情况下可能更稳定，因为它与当前策略保持一致。然而，这也可能导致它在探索和利用之间的权衡上不如Q-learning灵活。
   - Q-learning：Q-learning是一个off-policy算法，它通常更容易在某些情况下获得最优值函数，但可能在探索和利用之间的平衡上不够稳定。【例子是悬崖漫步更可能会掉下悬崖】

## Dyna-Q 算法

**基于模型的强化学习**是指在学习过程中，智能体（agent）能够建立一个环境模型，该模型可以预测环境的状态转移和奖励情况。代理根据这个模型进行规划和决策，从而选择最优的行动策略。

上一章中讨论的两种动态规划算法，即策略迭代和价值迭代，则是基于模型的强化学习方法，在这两种算法中环境模型是事先已知的（环境的状态转移函数和奖励函数）。本章即将介绍的 Dyna-Q 算法也是非常基础的基于模型的强化学习算法，不过它的环境模型是通过采样数据估计得到的。

1. 强化学习算法有两个重要的评价指标：一是算法在收敛后的策略在初始状态下的期望回报，二是样本复杂度，即算法在真实环境中采样的样本数量。
2. 基于模型的强化学习算法具有一个环境模型，这允许智能体额外与环境模型进行交互。这一特性通常使得它们比无模型的强化学习算法具有更低的样本复杂度，因为可以减少对真实环境样本的需求。
3. 尽管基于模型的强化学习算法在样本复杂度方面有优势，但环境模型可能不够准确，不能完全代替真实环境。因此，这些算法在策略的期望回报方面可能不如无模型的强化学习算法表现出色，因为它们的策略建立在模型的基础上，而非真实环境。

### Dyna-Q

Dyna-Q 算法是一个经典的基于模型的强化学习算法。如下图所示，Dyna-Q 使用一种叫做 Q-planning 的方法来基于模型生成一些模拟数据，然后用模拟数据和真实数据一起改进策略。Q-planning 每次选取一个曾经访问过的状态，采取一个曾经在该状态下执行过的动作 a，通过模型得到转移后的状态 s' 以及奖励 r，并根据这个模拟数据(s, a, r, s')，用 Q-learning 的更新方式来更新动作价值函数。

![img](/images/480.25b67b37.png)

Dyna-Q 算法的具体流程：

初始化$Q(s,a)$,初始化环境模型$M(s,a)$
for 序列$e=1\to E$ do:
	得到初始状态$s$

​	for $t=1\to T$ do:{
​		用$\epsilon$-贪婪策略根据$Q$选择当前状态$s$下的动作$a$
​		得到环境反馈的$r,s^{\prime}$
​		$Q(s,a)\gets Q(s,a)+\alpha[r+\gamma\max_{a^{\prime}}Q(s^{\prime},a^{\prime})-Q(s,a)]$
​		$M(s,a)\gets r,s^{\prime}$
​		for 次数 $n=1\to N$ do:{

​			随机选择一个曾经访问过的状态$s_m$ 
​			采取一个曾经在状态$s_m$下执行过的动作$a_m$
​			 $r_m,s_m^{\prime}\gets M(s_m,a_m)$
​			$Q(s_m,a_m)\gets Q(s_m,a_m)+\alpha[r_m+\gamma\max_{a^{\prime}}Q(s_m^{\prime},a^{\prime})-Q(s_m,a_m)]$

​		}end for
​		$s\gets s^{\prime}$

​	}end for

end for

可以看到，在每次与环境进行交互执行一次 Q-learning 之后，Dyna-Q 会做n次 Q-planning。其中 Q-planning 的次数N是一个事先可以选择的超参数，当其为 0 时就是普通的 Q-learning。值得注意的是，上述 Dyna-Q 算法是执行在一个离散并且确定的环境中，所以当看到一条经验数据(s, a, r, s')时，可以直接对模型做出更新，即$M(s,a)\gets r,s^{\prime}$。

### [代码实践](https://hrl.boyuai.com/chapter/1/dyna-q算法#63-dyna-q-代码实践)

略。
