---
title: 强化学习基础-时序差分算法
categories: 强化学习
date: 2023-10-11 20:34:00
mathjax: true
---

书接上一节[【动态规划算法】](https://www.dilettante258.cyou/2023/10/强化学习基础-动态规划算法/)

原链接：[动手学强化学习](https://hrl.boyuai.com/chapter/1/时序差分算法)

PDF课件：[值函数估计](https://boyuai.oss-cn-shanghai.aliyuncs.com/disk/动手学系列/动手学强化学习/课件pdf/3-值函数估计.pdf)	[无模型控制](https://boyuai.oss-cn-shanghai.aliyuncs.com/disk/动手学系列/动手学强化学习/课件pdf/4-无模型控制.pdf)

​		



动态规划算法适用于已知马尔可夫决策过程的情况，可以直接解出最优价值或策略。但在大部分情况下，马尔可夫决策过程的状态转移概率是未知的，这时就需要使用无模型的强化学习算法。无模型的强化学习算法不需要事先知道环境的奖励函数和状态转移函数，而是通过与环境交互采样数据来学习。

模型无关的强化学习直接从经验中学习值（value）和策略 （policy），而无需构建马尔可夫决策过程模型（MDP）。关键步骤：（1）估计值函数；（2）优化策略

无模型的强化学习算法基于时序差分（temporal difference，TD）学习，其中两个经典算法是Sarsa和Q-learning。Sarsa算法是一种在线策略学习算法，它使用当前策略下采样得到的样本进行学习。Q-learning算法是一种离线策略学习算法，它使用经验回放池将之前采样得到的样本收集起来再次利用。

离线策略学习使用历史数据进行学习，可以更好地利用历史数据，并具有更小的样本复杂度。因此，离线策略学习在实际应用中更为广泛。

## 值函数估计

在基于模型的强化学习（MDP）中，值函数能够通过动态规划计算获得
$$
\begin{aligned}
V^{\pi}(s)& =\mathbb{E}[R(s_0)+\gamma R(s_1)+\gamma^2R(s_2)+\cdots|s_0=s,\pi]  \\
&=R(s)+\gamma\sum_{s^{\prime}\in\mathcal{S}}P_{s\pi(s)}(s^{\prime})V^{\pi}(s^{\prime})
\end{aligned}
$$
 在模型无关的强化学习中，我们无法直接获得$𝑃_{𝑠𝑎}$和 𝑅 ，但是，我们拥有一系列可以用来估计值函数的经验或者说Episodes。

### 蒙特卡洛方法

蒙特卡洛方法（Monte-Carlo methods）是一类广泛的计算算法，依赖于重复随机抽样来获得数值结果。生活中处处都是MC方法。 
例如，计算圆的面积，围棋对弈中估计当前状态下的胜率。

### 蒙特卡洛价值估计

回顾：

- 累计奖励（return）是总折扣奖励
$$
  G_t=R_{t+1}+\gamma R_{t+2}+\cdots\gamma^{T-1}R_T
$$

- 值函数（value function）是期望累计奖励

$$
  \begin{aligned}
  V^{\pi}(s)& =\mathbb{E}[R(s_0)+\gamma   R(s_1)+\gamma^2R(s_2)+\cdots|s_0=s,\pi]  \\
  &=\mathbb{E}[G_t|s_t=s,\pi] \\
  &\simeq\frac1N\sum_{i=1}^NG_t^{(i)}
  \end{aligned}
$$

  上式使用策略𝜋从状态𝑠采样𝑁个片段，从黑盒中 计算平均累计奖励。

蒙特卡洛策略评估使用<font color=DodgerBlue>经验均值累计奖励</font>而不是<font color=DodgerBlue>期望累计奖励</font>。

目标：从策略 𝜋下的经验片段学习$𝑉^𝜋$
$$
S_0^{(i)}\underset{R_1^{(i)}}{\operatorname*{\overset{a_0^{(i)}}{\operatorname*{\longrightarrow}}}}S_1^{(i)}\underset{R_2^{(i)}}{\operatorname*{\overset{a_1^{(i)}}{\operatorname*{\longrightarrow}}}}S_2^{(i)}\underset{R_3^{(i)}}{\operatorname*{\overset{a_2^{(i)}}{\operatorname*{\longrightarrow}}}}S_3^{(i)}...S_T^{(i)}\boldsymbol{\sim}\pi
$$
在一个片段中的<font color=DodgerBlue>每个时间步长𝑡</font>的<font color=DodgerBlue>状态𝑠</font>都被访问

- 增量计数器 𝑁(𝑠) ← 𝑁(𝑠) + 1

- 增量总累计奖励 𝑆(𝑠) ← 𝑆(𝑠) + 𝐺𝑡

- 价值被估计为累计奖励的均值 𝑉(𝑠) = 𝑆(𝑠)/𝑁(𝑠)

- 由大数定率有
$$
  V(s)\to V^{\pi}(s)\mathrm{~as~}N(s)\to\infty
$$


增量蒙特卡洛更新

- 每个片段结束后逐步更新𝑉(𝑠) 
- 对于每个状态$𝑆_𝑡$和对应累计奖励$𝐺_t$

$$
\begin{aligned}&N(S_t)\leftarrow N(S_t)+1\\&V(S_t)\leftarrow V(S_t)+\frac1{N(S_t)}\big(G_t-V(S_t)\big)\end{aligned}
$$

- 对于非稳定的问题（即，环境会随时间发生变化），我们可以跟踪一个现阶段的平均值（即，不考虑过久之前的片段）
$$
  V(S_t)\leftarrow V(S_t)+\alpha(G_t-V(S_t))
$$
  上式可化为$(1- α )V(S_t) + αG_t$



**小结**
$$
\begin{aligned}\text{思路:}\quad&V(S_t)\simeq\frac1N\sum_{i=1}^NG_t^{(i)}\\ \text{实现:}\quad&V(S_t)\leftarrow V(S_t)+\alpha\big(G_t-V(S_t)\big)\end{aligned}
$$

- 蒙特卡洛方法：直接从经验片段进行学习
- 蒙特卡洛是模型无关的：未知马尔可夫决策过程的状态转移/奖励，直接通过采样的方式去逼近，通过大数定理的方式去逼近这样一个期望值。
- 蒙特卡洛必须从完整的片段中进行学习：没有使用bootstrapping的方法(居于碎片自举法)
- 蒙特卡洛采用最简单的思想：值（value）= 平均累计奖励（mean  return）
- 注意：只能将蒙特卡洛方法应用于有限长度的马尔可夫决策过程中。即，<u>所有的片段都有终止状态</u>。不然只能用时序差分算法。



## 重要性采样

重要性采样是在机器学习当中的一个非常重要的概念，因为在机器学习当中我们往往需要去优化的是在一定数据分布上的一个损失函数。而如果我们拿到的训练数据，他是在另外一个数据分布下面的话，我们就可以考虑重要性采样。
我们首先通过数学的方法来分析一下重要性采样它的本质原理。

估计一个不同分布的期望：
$$
\begin{aligned}
\mathbb{E}_{x\sim p}[f(x)]& =\int_xp(x)f(x)dx  \\
&=\int_xq(x)\frac{p(x)}{q(x)}f(x)dx \\
&=\mathbb{E}_{x\sim q}\left[\frac{p(x)}{q(x)}f(x)\right]
\end{aligned}
$$
将每个实例的权重重新分配为$\beta(x)=\frac{p(x)}{q(x)}$



<span style="background:yellow">待补充待补充待补充</span>

## 时序差分学习（Temporal Difference Learning）

无模型学习中需要找到一个当前目标，学习的目标主要是当前状态的真正地V(s)是多少。要估计它，可以


$$
G_t=R_{t+1}+\gamma R_{t+2}+\gamma^2R_{t+3}+\cdots=R_{t+1}+\gamma V(S_{t+1})\\V(S_t)\leftarrow V(S_t)+\alpha(R_{t+1}+\gamma V(S_{t+1})-V(S_t))
$$
$R_{t+1}$:观测值		$V(S_{t+1})$:对未来的猜测



时序差分方法直接<font color=deepskyblue>从经验片段中进行学习</font>

时序差分是<font color=coral>模型无关</font>的：不需要预先获取马尔可夫决策过程的状态转移/奖励

通过bootstrapping，时序差分从<font color=deepskyblue>不完整的片段中学习</font>

时序差分更新当前预测值使之接近估计累计奖励（观测值+对未来的猜测，<font color=coral>非真实值</font>）

## 蒙特卡洛 vs. 时序差分（MC vs. TD)

相同的目标：<font color=coral>从策略𝜋下的经验片段学习$𝑉^𝜋$</font>

- 增量地进行每次蒙特卡洛过程（<font color=coral>MC</font>）

  - 更新值函数$𝑉(𝑆_𝑡)$使之接近<font color=deepskyblue>准确累计奖励$𝐺_t$</font>


$$
  V(S_t)\leftarrow V(S_t)+\alpha\big(G_t-V(S_t)\big)
$$

- 最简单的时序差分学习算法（TD）：

  - 更新$V(S_t)$使之接近估计累计奖励$R_{t+1}+\gamma V(S_{t+1})$


$$
  V(S_t)\leftarrow V(S_t)+\alpha(R_{t+1}+\gamma V(S_{t+1})-V(S_t))
$$

  - 时序差分目标：$R_{t+1}+\gamma V(S_{t+1})$

  - 时序差分误差：$\delta_{t}=R_{t+1}+\gamma V(S_{t+1})-V(S_{t})$

## 偏差（Bias）/方差（Variance）的权衡

两者目标相同：从策略 𝜋下的经验片段学习(估计)$𝑉^𝜋$

偏差：

1. MC增量地更新$V(S_t)$逼近准确累计奖励$G_t$			无偏估计
2. TD更新$V(S_t)$逼近估计累计奖励$R_{t+1}+\gamma V^{\pi}(S_{t+1})$	有偏估计（观测值+对未来的猜测）

> MC中累计奖励$G_t=R_{t+1}+\gamma R_{t+2}+\cdots+\gamma^{T-1}R_T$是$V^{\pi}(S_t)$的无偏估计	$\mathbb{E}\left[G_t \right]=V^{\pi}(s_t)$
>
> 时序差分<font color=deepskyblue>真实目标 </font>$R_{t+1}+\gamma V^{\pi}(S_{t+1})$是$V^{\pi}(S_t)$的无偏估计
>
> 时序差分<font color=deepskyblue>目标</font><a name="时序差分目标"> </a>$R_{t+1}+\gamma V(S_{t+1})$是$V^{\pi}(S_t)$的有偏估计（观测值+对未来的猜测）

> **无偏估计：** 无偏估计是指在进行参数估计时，估计值的期望与真实参数值相等。
> **有偏估计：** 有偏估计是指在进行参数估计时，估计值的期望与真实参数值不相等。换句话说，有偏估计会系统地高估或低估参数的真实值。

时序差分目标有比累计奖励更低的方差，更大的偏差

- MC累计奖励——取决于多步随机动作，多步状态转移和多步奖励
- 时序差分目标——取决于单步随机动作，单步状态转移和单步奖励

$G_t$每一步的采样都有不确定性，因此不同采样的$G_t$可能方差很大。TD相比之下不确定性来自于$R_{t+1}$的采样，学习更加稳定。
TD<a href="#时序差分目标">目标</a>是有偏的，而MC是无偏的。

## 蒙特卡洛(MC)和时序差分(TD) 的优缺点

- 时序差分:能够在知道最后结果之前进行学习

  - 时序差分能够在<font color=deepskyblue>每一步之后进行在线学习</font>
  - 蒙特卡洛<font color=deepskyblue>必须等待片段结束</font>，直到累计奖励已知

  

- 时序差分:能够无需最后结果地进行学习

  - 时序差分能够从不完整的序列中学习
  - 蒙特卡洛只能从完整序列中学习
  - 时序差分在连续（<font color=coral>无终止的</font>）环境下工作
  - 蒙特卡洛只能在片段化的 （<font color=coral>有终止的</font>）环境下工作



<img src="/images/image-20231011221659410.png" alt="image-20231011221659410" style="zoom: 80%;" />

## 多步时序查分学习

对于有时间约束的情况，我们可以跳过𝑛步预测的部分，直接进 入模型无关的控制

定义𝑛步累计奖励
$$
G_t^{(n)}=R_{t+1}+\gamma R_{t+2}+\cdots+\gamma^{n-1}R_{t+n}+\gamma^nV(S_{t+n})
$$
𝑛步时序差分学习
$$
V(S_t)\leftarrow V(S_t)+\alpha\left(G_t^{(n)}-V(S_t)\right)
$$





## SARSA

在当前策略下执行的每个（<font color=coral>状态-动作-奖励-状态-动作</font>）元组
所以的它简称的是(S-A-R-S'-A') 的这样一个5元组，贪婪算法来选取<font color=deepskyblue>*在某个状态下*</font><font color=teal>动作价值</font>最大的那个动作，即$\arg\max_aQ(s,a)$。用贪婪算法根据动作价值选取动作来和环境交互，再根据得到的数据用时序差分算法更新动作价值估计。

![image-20231012212139248](images/image-20231012212139248.png)

 SARSA更新状态-动作值函数为
$$
Q(s,a)\gets Q(s,a)+\alpha(r+\gamma Q(s^{\prime},a^{\prime})-Q(s,a))
$$
这个简单的算法存在两个需要进一步考虑的问题。

- SARSA算法是一种时序差分学习算法，通过不断地更新状态-动作对的价值函数来学习最优策略。在传统的SARSA算法中，为了准确地估计状态价值函数，需要使用大量的样本进行更新。然而，作者指出，实际上我们可以使用较少的样本来评估策略，然后进行策略的更新。这是因为策略提升可以在策略评估尚未完全进行的情况下进行，即可以在部分样本的基础上进行策略的改进。这种思想与广义策略迭代的思想相符合，广义策略迭代是一种通过交替进行策略评估和策略改进的方法来逐步优化策略的过程。因此，作者认为在SARSA算法中，我们可以忽略使用大量样本的要求，而只使用一些样本进行策略评估和更新，从而简化算法的实现。
- 某些状态动作对(s,a)可能永远没有在序列中出现，以至于无法对其动作价值进行估计，进而无法保证策略提升后的策略比之前的好。简单常用的解决方案是不再一味使用贪婪算法，而是采用一个ε-greedy贪婪策略

![image-20231013174604626](images/image-20231013174604626.png)

初始化$Q(s,a)$

for 序列$e=1\to E$  do:{
	得到初始状态$s$
	用$\epsilon$-greedy 策略根据$Q$选择当前状态$s$下的动作$a$
 	for 时间步$t=1\to T$ do :{
		得到环境反馈的$r,s^{\prime}$
		用$\epsilon$-greedy 策略根据$Q$选择当前状态$s^{\prime}$下的动作$a^{\prime}$
		$Q(s,a)\gets Q(s,a)+\alpha[r+\gamma Q(s^{\prime},a^{\prime})-Q(s,a)]$
		$s\gets s^{\prime},a\gets a^{\prime}$}
		end for
end for









在线策略时序差分控制（on-policy TD control）使用当前策略进行动作采样。即，SARSA算法中的两个“A”都是由当前策略选择的

## Q-learning 算法

是一种离线策略（off-policy）[^非Offline]学习方法， 学习 <u>状态-动作值函数 𝑄(𝑠, 𝑎) ∈ ℝ</u>，不直接优化策略本身。

> Off-policy学习方法允许智能体在学习过程中使用与其当前策略不一致的数据来评估和改进策略。
>
> off-policy学习方法使用两个不同的策略：目标策略和行为策略。目标策略是智能体希望学习的最优策略，而行为策略是智能体在与环境交互时实际采取的策略。智能体通过从行为策略中收集的经验数据来评估和改进目标策略。

<img src=/images/image-20231013181954191.png" alt="image-20231013181954191" style="zoom:50%;" />

### 离线策略学习

什么是离线策略学习

- 目标策略 $\pi(a|s)$ 进行值函数评估	$(V^{\pi}(s)$或$Q^\pi(s,a))$ 
- 行为策略 $\mu(a|s)$ 收集数据		$\quad\{s_1,a_1,r_2,s_2,a_2,...,S_T\}{\sim}\mu$

为什么使用离线策略学习[^来源Chatgpt]

- 数据重用：离线策略学习可以利用已有的数据集进行学习，而不需要在每次策略改进时重新采集数据。这样可以节省时间和资源，并且可以更高效地进行策略评估和改进。
- 策略评估和改进的分离：离线策略学习将策略评估和策略改进解耦。智能体可以根据已有的经验数据来评估不同的策略，而不需要实际执行这些策略。这使得策略评估和改进的过程更加灵活和独立。
- 探索性策略：离线策略学习可以使用探索性策略来更有效地探索环境，平衡探索（exploration）和利用（exploitation）。探索性策略可以是一种随机策略或者是基于先前经验的策略。通过使用探索性策略，离线学习可以更好地发现和利用环境中的潜在奖励信号，从而提高学习效果。
- 策略偏移：离线策略学习可以处理策略偏移的问题。策略偏移指的是目标策略和行为策略之间的差异。通过使用重要性采样等技术，离线学习可以校正由于策略偏移引起的估计偏差，从而更准确地评估和改进目标策略。

### Q 学习

无需重要性采样（为什么？）

- 每次更新需要的经验片段是(S-A-R-S'-A')
- s,a,r,s是当前更新的策略是确定的，可认为分布无关
- 后续动作a是由目标策略选择的，分布一致

根据行为策略选择动作$a_t{\sim}\mu(\cdot|s_t)$

根据目标策略选择后续动作$a^{\prime}{}_{t+1}{\sim}\pi(\cdot|s_t)$

