---
title: 暂退法(Dropout)
categories: Pytorch
date: 2023-09-25 23:20:00
mathjax: true
---

过拟合是机器学习和深度学习中一个常见的问题，它发生在模型在训练数据上表现很好，但在未见过的新数据上表现不佳的情况下。在权重衰退一节中，我们介绍了通过惩罚权重的$L_2$范数来正则化统计模型的经典方法。 

网络过拟合可能因为我们的网络太过于复杂，L2正则化方法就是通过使参数变小，进而使模型变得简单的方法。dropout方法原理类似，只不过它不是减少权值，而是随机的删除某些节点，使得模型的网络结构变得简单，起到正则化的效果。

1. **正则化方法**:
   - 通过惩罚权重的范数来正则化模型。
   - 从概率角度看，权重的先验假设是均值为0的高斯分布。
2. **过拟合**:
   - 线性模型在特征多样本少时容易过拟合，但在样本量大时不易过拟合。
   - 线性模型忽略了特征之间的交互作用，有较高的偏差但较低的方差。
   - 深度神经网络考虑了特征间交互，位于偏差-方差谱的另一端，有可能在样本多时仍然过拟合。
   - 深度网络的泛化性质是一个未解决的问题。
3. **偏差-方差权衡**:
   - 线性模型有高偏差、低方差，只能表示一小类函数。
   - 深度神经网络能学习特征间交互，有低偏差、高方差，泛化差距可能较大。
4. **扰动的稳健性与暂退法（Dropout）**:
   - 扰动的稳健性表明好的模型应对输入的微小变化不敏感。
   - 暂退法是一种在训练过程中向网络的每一层注入噪声的方法，可以增强模型的泛化能力。
   - 暂退法的原理是通过随机丢弃一些神经元，破坏网络层之间的共适应性。
   - 通过按保留（未丢弃）的节点的分数进行规范化来消除每一层的偏差。
   - 在实际应用中，暂退法已经成为训练神经网络的常用技术。