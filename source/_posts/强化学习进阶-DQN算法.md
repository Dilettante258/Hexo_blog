---
title: 强化学习进阶-DQN算法
categories: 强化学习
date: 2023-10-14 23:08:00
mathjax: true
---

原链接：[动手学强化学习](https://hrl.boyuai.com/chapter/2/dqn算法)

PDF课件：

---

当状态或者动作连续的时候，就有无限个状态动作对，我们无法使用表格形式来记录各个状态动作对的Q值。

对于这种情况，我们需要用函数拟合的方法来估计Q值，即将这个复杂的Q值表格视作数据，使用一个参数化$Q_\theta$的函数来拟合这些数据。很显然，这种函数拟合的方法存在一定的精度损失，因此被称为近似方法。 DQN 算法可以用来解决连续状态下离散动作的问题。

## CartPole 环境

CartPole 是一个常用于强化学习研究和实验的虚拟环境。它是由OpenAI开发的，用于测试和训练强化学习算法的经典问题之一。在CartPole环境中，有一个可以水平移动的小车（cart），上面有一根可以倾斜的杆（pole）。任务的目标是使杆保持平衡在竖直位置上，防止它倒下。

CartPole环境具有以下特征：

1. **状态空间（State Space）**：CartPole环境的状态由四个连续值组成，分别是小车的位置、小车的速度、杆的角度和杆的角速度。这些状态信息用于描述环境的当前状态。
2. **动作空间（Action Space）**：代理（Agent）可以执行两个离散的动作，即向左推动小车或向右推动小车。
3. **奖励（Reward）**：在每个时间步，代理会收到一个正的奖励，只要杆保持在垂直位置上。任务的目标是最大化累积奖励，即保持杆的平衡尽可能长的时间。
4. **终止条件（Termination）**：任务会在杆倾斜角度超过一定阈值或者小车移动出一定范围时终止。当任务终止时，代理可以重新开始游戏。

## DQN

DQN（Deep Q-Network）是一种强化学习算法，最初由DeepMind于2013年提出，用于解决离散动作空间的马尔可夫决策过程（MDP）问题。DQN的主要创新是将深度神经网络与Q-learning算法相结合，以解决具有高维状态空间的问题。**【解决 Q-Learning 无法解决的维度爆炸问题】**

以下是DQN算法的关键要点和组成部分：

1. **Q-Learning**：DQN建立在Q-learning算法的基础上。Q-learning是一种强化学习算法，用于估计每个状态动作对的值函数（Q值函数），表示在给定状态下采取某个动作的预期累积奖励。DQN旨在使用深度神经网络来逼近Q值函数，以处理高维状态空间。
2. **深度神经网络**：得到动作价值函数Q(s,a)，由于状态每一维度的值都是连续的，无法使用表格记录，因此一个常见的解决方法便是使用**函数拟合**（function approximation）的思想。由于神经网络具有强大的表达能力，因此我们可以用一个神经网络来表示函数Q。DQN使用一个深度神经网络来估计Q值函数。神经网络的输入是环境的状态，输出是每个可能动作的Q值。通过训练神经网络，DQN试图学习一个接近最优Q值函数的近似。
3. **经验回放（Experience Replay）**：为了提高算法的稳定性和样本效率，DQN引入了经验回放机制。它会将代理在过去交互中观察到的状态、动作、奖励和下一个状态存储在一个经验回放缓冲区中，然后随机抽样这些经验样本用于训练神经网络。
4. **目标网络（Target Network）**：为了增强算法的稳定性，DQN使用两个神经网络：一个是主要的Q网络，另一个是目标Q网络。目标Q网络的参数是固定的，并且不经常更新，而主要的Q网络用于实际的Q值估计。这有助于减少训练过程中的估计偏差。

DQN的训练目标是最小化Q值函数的均方误差，以使其逼近真实的Q值函数。通过反复迭代这个过程，DQN可以学会在不同环境中采取适当的动作，以最大化累积奖励。

如果动作是离散（有限）的，除了可以采取动作连续情况下的做法，我们还可以只将状态输入到神经网络中，使其同时输出每一个动作的值（因为动作是有限的）。

> DQN和Q-learning通常只能处理离散动作的情况，主要是因为它们的核心机制和数学基础是基于离散动作空间的。
>
> 1. Q-learning和DQN算法的核心是Q-值函数，它用于估计在给定状态下采取每个可能动作的值。在离散动作空间中，动作可以被看作是离散的，每个动作对应一个Q值。
> 2. 在离散动作空间中，可以轻松地选择具有最高Q值的动作作为贪婪策略的一部分。这使得在每个时间步中选择最佳动作相对简单。
> 3. Q-learning使用贝尔曼方程来更新Q值，该方程基于最优动作的选择。*在连续动作空间中，寻找最优动作需要进行优化过程，这会增加计算复杂性。*

### DQN的损失函数

Q-learning 的更新规则是使$Q(s,a)$和TD 目标$r+\gamma\max_{a^{\prime}\in\mathcal{A}}Q(s^{\prime},a^{\prime})$靠近。将 Q 网络的损失函数构造为均方误差的形式
$$
\omega^*=\arg\min_\omega\frac{1}{2N}\sum_{i=1}^N\left[Q_\omega\left(s_i,a_i\right)-\left(r_i+\gamma\max_{a'}Q_\omega\left(s'_i,a'\right)\right)\right]^2
$$
损失函数是要最小化<span style="background:yellow">predicted Q value</span>和<span style="background:yellow">target Q value</span>之间的error。$Q(s,a)$ 是predicted Q value，它就是神经网络的输出，表示的是状态为s，动作为a的时候神经网络的预测值。$r+\gamma\max_{a^{\prime}\in\mathcal{A}}Q(s^{\prime},a^{\prime})$ 是target Q value，它是指当选择动作a时**实际观测到的值**。当agent选择一个动作，它会得到一个reward，然后我们可以它存下来，我们同样可以把every action after that的discounted reward存下来。它们两的和就是target Q value，是给一个动作实际获得的所有reward。

可以用predicted Q value去做回归问题，然后用和普通神经网络里一样的反向传播去训练损失函数，然后根据损失函数去训练神经网络使得predicted Q value尽量接近target Q value。[^深度强化学习DQN]

### 独立同分布

> "独立同分布" 是统计学和概率论中的一个重要概念，通常简称为"i.i.d."。
> **独立（Independent）**是说当我们说两个或多个随机变量是独立的时，意味着它们之间没有相互影响或相关性。**同分布（Identically Distributed）**是说当我们说两个或多个随机变量是同分布的时，意味着它们来自相同的概率分布。这意味着它们具有相同的可能取值范围和概率分布函数。
> "独立同分布" 表示一组随机变量是相互独立且来自相同的概率分布。

在一般的有监督学习中，假设训练数据是独立同分布的是为了简化建模和分析的过程，并且允许应用许多统计和机器学习技术。假设有以下几点原因：

1. **数学方便性：** 假设训练数据是独立同分布的，可以简化数学模型的推导和分析。这使得我们可以使用概率论和统计学的工具来推导模型的性质、进行参数估计和进行假设检验，从而更容易理解和解释模型的行为。
2. **泛化能力：** 如果训练数据是从相同的分布中独立采样的，那么模型在训练数据上学到的规律更有可能泛化到未见过的新数据。这是因为模型通过学习来自相同分布的样本可以更好地适应未来来自相同分布的数据。如果数据不是独立同分布的，模型可能会过度拟合训练数据，导致泛化性能下降。
3. **统计效率：** 在独立同分布的假设下，我们可以利用大量的统计理论来估计模型参数、计算置信区间和假设检验，这有助于更好地了解模型在不同情况下的性能和可靠性。[^Chatgpt]

### 经验回放

我们每次训练神经网络的时候从训练数据中随机采样一个或若干个数据来进行梯度下降，随着学习的不断进行，每一个训练数据会被使用多次。在原来的 Q-learning 算法中，每一个数据只会用来更新一次值。

为了更好地将 Q-learning 和深度神经网络结合，DQN 算法采用了**经验回放**（experience replay）方法，具体做法为维护一个**回放缓冲区**，将每次从环境中采样得到的四元组数据（状态、动作、奖励、下一状态）存储到回放缓冲区中，训练 Q 网络的时候再从回放缓冲区中随机采样若干数据来进行训练。

作用：

1. **使样本满足独立假设**。在 MDP 中交互采样得到的数据本身不满足独立假设，因为这一时刻的状态和上一时刻的状态有关。非独立同分布的数据对训练神经网络有很大的影响，会使神经网络拟合到最近训练的数据上。采用经验回放可以打破样本之间的相关性，让其满足独立假设。
2. **提高样本效率**。每一个样本可以被使用多次，十分适合深度神经网络的梯度学习。

### 目标网络

DQN 算法最终更新的目标最小化[DQN的损失函数](#DQN的损失函数)，由于 TD 误差目标本身就包含神经网络的输出，因此在更新网络参数的同时目标也在不断地改变，这非常容易造成神经网络训练的不稳定性。为了解决这一问题，DQN 便使用了**目标网络**（target network）的思想：既然训练过程中 Q 网络的不断更新会导致目标不断发生改变，不如暂时先将 TD 目标中的 Q 网络固定住。为了实现这一思想，我们需要利用两套 Q 网络。

1. 原来的训练网络$Q_w(s,a)$ ，用于计算原来的损失函数中的项$Q_w(s,a)$ ，并且使用正常梯度下降方法来进行更新。

2. <font color=DodgerBlue>**目标网络**</font> $Q_{w^-}(s,a)$，用于计算原先损失函数中的 $r+\gamma\max_{a^{\prime}\in\mathcal{A}}Q_{w^-}(s^{\prime},a^{\prime})$ 项，其中 $w^-$ 表示目标网络中的参数。

如果两套网络的参数随时保持一致，则仍为原先不够稳定的算法。为了让更新目标更稳定，目标网络并不会每一步都更新。具体而言，目标网络使用训练网络的一套较旧的参数，训练网络 $Q_w(s,a)$ 在训练中的每一步都会更新，而目标网络的参数每隔步才会与训练网络同步一次，即 $\omega^{-}\leftarrow\omega$。这样做使得目标网络相对于训练网络更加稳定。

### DQN 算法的具体流程

用随机的网络参数$\omega$初始化网络$Q_{\omega}(s,a)$
复制相同的参数$\omega^{-}\gets\omega$来初始化目标网络$Q_{\omega^{\prime}}$
初始化经验回放池$R$
**for** 序列$e=1\to E$ do
	 获取环境初始状态$s_1$
	 **for** 时间步$t=1\to T$ do
	 	根据当前网络$Q_{\omega}(s,a)$以**$\epsilon$-贪婪策略**选择动作$a_t$，进行探索
	 	执行动作$a_t$,获得回报$r_t$,环境状态变为$s_{t+1}$
 		将$(s_t,a_t,r_t,s_{t+1})$存储进回放池$R$中	# 收集数据
 		若$R$中数据足够,从$R$中***采样***$N$个数据$\{(s_i,a_i,r_i,s_{i+1})\}_{i=1,\ldots,N}$	# 采样
 		对每个数据, 用目标网络计算$y_i=r_i+\gamma\max_aQ_{\omega^-}(s_{i+1},a)$
 		最小化目标损失$L=\frac1N\sum_i(y_i-Q_\omega(s_i,a_i))^2$,以此更新当前网络$Q_{\omega}$
 		每 C 次迭代(更新Q函数网络) 更新一次目标网络		# 更新网络
	**end for**
**end for**

## DQN 代码实践

[链接](https://hrl.boyuai.com/chapter/2/dqn算法#74-dqn-代码实践)

原代码有问题，不能正常运行。

---
Ref:


1. [动手学强化学习]: https://hrl.boyuai.com/chapter/2/dqn算法

2. [^Chatgpt]: https://chat.openai.com/

2. [^深度强化学习DQN]: https://blog.csdn.net/qq_22866291/article/details/127517205 

