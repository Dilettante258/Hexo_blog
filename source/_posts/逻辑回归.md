---
title: 逻辑回归
categories: Python
date: 2023-07-26 21:00:00
mathjax: true
---

最简单的分类器：二元分类器（输出值为0或者1）。
### 模型的可解释性
商用模型需要具备良好的可解释性，不然客户不会买账。有些模型天生可以说故事，比如决策树模型；而有些模型则是个黑匣子，你根本不知道里面发生了什么，比如随机森林模型。即便随机森林只是决策树模型的一个推广，在可解释性方面却有天壤之别。复杂的模型通常都很难解释，因此如果你没有足够的时间去解释一个复杂模型的结果，可以考虑牺牲一些准确度，把模型稍作简化。
### 可扩展性

#### 制约模型可扩展性的是模型的成本，现实中会考虑以下三方面的成本。
1. 学习时间：也就是模型的训练时间。
2. 得分时间：也就是模型的预测时间，给定一个数据和已经训练好的模型，新用户要多久才能得到一个预测值？
3. 模型存储：模型运行时要占用多大的内存？

#### 监督学习算法要注意的几点
1. 模型的复杂程度经常与它的精度成正比。简单的模型可能具有更好的可解释性，但可能不会有令人满意的预测精度。
2. 没有万能算法，每个问题都要根据其自身的特点选择最适合的模型。
3. 诸多条件会限制着你对可用算法的选择，包括数据量大小，项目成本和时间成本等。


#### 一个好的模型应该能够做到这几点：
1. 特征工程：找出最有用的特征变量并且知道如何正确处理和使用它们。
2. 交互预测：模型需要具备迅速的用户响应能力，用户鼠标按下的一瞬间，模型要即时地预测和输出。
3. 精准定价。

#### 逻辑回归的模型背后
逻辑回归的微妙之处就在于输出值是介于0和1之间的概率值。奥妙就在于，数据的特征矩阵被某一个神奇的函数巧妙地转换成了一个严格位于$[0, 1]$之间的数值反逻辑函数（inverse-logit function）其定义域为全体实数，而值域为$[0, 1]$。

逻辑回归一个明确的定义了：
对于点击模型来说，用户i点击卖鞋广告的逻辑概率可以用该用户网页访问历史的特征变量（也就是之前对URL预处理得到的稀疏矩阵）的线性组合来表示。


#### 牛顿法
牛顿法是一种在实数域和复数域上近似求解方程的方法。方法使用函数f (_x_)的泰勒级数的前面几项来寻找方程$f(x) = 0$的根。牛顿法最大的特点就在于它的收敛速度很快（二阶收敛）。缺点是牛顿法是一种迭代算法，每一步都需要求解目标函数的Hessian矩阵的逆矩阵，计算比较复杂。
#### 随机梯度下降法（Stochastic Gradient Descent，SGD）
梯度下降法是最早最简单，也是最为常用的最优化方法。梯度下降法实现简单，当目标函数是凸函数时，梯度下降法的解是全局解。一般情况下，其解不保证是全局最优解，梯度下降法的速度也未必是最快的。*梯度下降法的优化思想是用当前位置负梯度方向作为搜索方向，因为该方向为当前位置的最快下降方向，所以也被称为是”最速下降法“。最速下降法越接近目标值，步长越小，前进越慢。*
随机梯度下降最小化每条样本的损失函数，虽然不是每次迭代得到的损失函数都向着全局最优方向， 但是大的整体的方向是向全局最优解的，最终的结果往往是在全局最优解附近，适用于大规模训练样本情况。
梯度下降法的缺点：
1. 靠近极小值时收敛速度减慢，如下图所示；
2. 直线搜索时可能会产生一些问题；
3. 可能会“之字形”地下降。
关于牛顿法和梯度下降法的效率对比：
从本质上去看，牛顿法是二阶收敛，梯度下降是一阶收敛，所以牛顿法就更快。如果更通俗地说的话，比如你想找一条最短的路径走到一个盆地的最底部，梯度下降法每次只从你当前所处位置选一个坡度最大的方向走一步，牛顿法在选择方向时，不仅会考虑坡度是否够大，还会考虑你走了一步之后，坡度是否会变得更大。所以，可以说牛顿法比梯度下降法看得更远一点，能更快地走到最底部。（牛顿法目光更加长远，所以少走弯路；相对而言，梯度下降法只考虑了局部的最优，没有全局思想。）
根据wiki上的解释，从几何上说，牛顿法就是用一个二次曲面去拟合你当前所处位置的局部曲面，而梯度下降法是用一个平面去拟合当前的局部曲面，通常情况下，二次曲面的拟合会比平面更好，所以牛顿法选择的下降路径会更符合真实的最优下降路径。


### 模型评价
#### 操作者特征曲线面积（ROC面积）
操作者特征曲线（也就是ROC曲线）结合了阈值和两个阳性值指标。该曲线图的纵轴代表模型的真阳性值，横轴代表模型的伪阳性值。给定一个逻辑回归模型和一个阈值，模型分类效果的真阳性值和伪阳性值都可以方便地计算出来，并可以绘制成一个二元平面上的点。阈值可以在-∞和∞之间连续变动，因此代表（真阳性值，伪阳性值）的点也可以连续变动，既而形成一条曲线。这条曲线就称作操作者特征曲线，它描绘了模型的分类效果与阈值的变动关系。该曲线到横轴之间的面积通常称作操作者特征曲线面积（AUC），它是评价一个分类模型效果的经典指标，因此也可以用来比较两个分类模型孰优孰劣。
#### 累积提升图
在直销行业，建模者较多地使用累积提升图判断一个模型到底有没有用。模型有用的最低标准是，它的效果至少应该好于随机猜测。
模型产品化的问题
在建模过程中，干扰因子的影响十分复杂却又至关重要。解决“干扰因子”影响的办法就是把该因子本身作为预测变量加入到模型当中。
假设要将逻辑回归模型用于分类问题，那么我们已经知道数据的实际输出是一个二元值（0, 1）， 而逻辑回归的模型输出是一个介于0和1之间的概率值。为了将逻辑回归模型应用到没有标签值的样本，我们需要将逻辑回归的实际输出（一系列概率值）转换成二元值（0和1）。为了最小化模型的误分率，我们需要在这个转换过程中选择一个合适的阈值。比如说阈值为0.5，那么只要预测概率值大于0.5， 预测标签值就为1（代表被点击）；反之则为0。

#### 几种数据

- 提升度
提升度指的是分类模型的预测精度相比随机猜测模型的提升幅度。
- 准确度
模型的准备度指的是所有被正确预测的类别（包括阳性类和阴性类）。
- 精确度
模型的精度指的是模型的真阳性值/所有的阳性值。也就是说，在所有被预测为阳性的类中，其真实为阳性的比例。
- 召回率
召回率指的是模型的真阳性值/（模型的真阳性值 + 模型的假阴性值）。也就是，在所有应该被预测为阳性的类中，其真实被预测为阳性的比例。
- F得分
之前我们没有提到任何关于F得分的内容，它是精度和召回度的调和平均值，其形式为：(2 * 精确度 * 召回度) / (精确度 + 召回度)。可以看出，它综合考虑到了精确度和召回度的不同特性。F得分还有很多变种，其区别主要在于精确度和召回度的权重不同。

#### 三种评价标准
评价或比较模型输出的实际概率值的精确程度，可以用以下三种评价标准。
- 均方误差
之前关于线性回归的章节中提到过均方误差的概念，它指的是实际值和预测值之间的平均平方距离。
- 根均方误差
顾名思义，根均方误差就是均方误差的平方根。
- 平均绝对离差
与均方误差不同的是，平均绝对离差计算的是预测值和实际值之间的绝对值距离，而不是平方距离。

AUC（接受者操作特征曲线下面积）要比提升曲线更适合用作模型比较。提升曲线的一大特征就是：“基率不变性。”比如说，如果模型将用户点击率从1%提升到2%，其提升率为100%；而如果从4%提升到7%，其提升率反而小于100%。然而，很明显后者的提升效率更高。从这一点来看，AUC要更加适用于模型的比较。

利润标准：反映我们想要的最优结果——最大化的利润。为了克服这个缺点，通常的做法是应用A/B测试优化法。