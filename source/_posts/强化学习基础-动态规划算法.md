---
title: 强化学习基础-动态规划算法
categories: 强化学习
date: 2023-10-10 18:58:00
mathjax: true
---

**动态规划**（dynamic programming）能够高效解决一些经典问题，例如背包问题和最短路径规划。动态规划的基本思想是将待求解问题分解成若干个子问题，先求解子问题，然后从这些子问题的解得到目标问题的解。动态规划会保存已解决的子问题的答案，在求解目标问题的过程中，需要这些子问题答案时就可以直接利用，避免重复计算。

## 最优价值函数

对状态s来说的最优价值函数是所有策略可获得的最大可能折扣奖励的和

$V^{*}(s)=\max _{\pi} V^{\pi}(s)$

最优价值函数的Bellman等式

$ V^{*}(s)=R(s)+\max _{a \in A} \gamma \sum_{s^{\prime} \in S} P_{s a}\left(s^{\prime}\right) V^{*}\left(s^{\prime}\right) $

最优策略

$ \pi^{*}(s)=\arg \max _{a \in A} \sum_{s^{\prime} \in S} P_{s a}\left(s^{\prime}\right) V^{*}\left(s^{\prime}\right) $

对状态s和策略π

$ V^{*}(s)=V^{\pi^{*}}(\mathrm{~s}) \geq V^{\pi}(s) $



而价值函数和策略相关
$$
\begin{aligned}V^{\pi}(s)&=R(s)+\gamma\sum_{s'\in S}P_{s\pi(s)}(s')V^{\pi}(s')\\\pi(s)&=\arg\max_{a\in A}\sum_{s'\in S}P_{sa}(s')V^{\pi}(s')\end{aligned}
$$
可以对最优价值函数和最优策略执行迭代更新（基于动态规划的强化学习算法），有两种方式：

- **策略迭代**（policy iteration）
- **价值迭代**（value iteration）

其中，策略迭代由两部分组成：

- **策略评估**（policy evaluation）：使用贝尔曼期望方程来得到一个策略的状态价值函数，这是一个动态规划的过程
- **策略提升**（policy improvement）：直接使用贝尔曼最优方程来进行动态规划，得到最终的最优状态价值

动态规划算法要求事先知道环境的状态转移函数和奖励函数，也就是需要知道整个马尔可夫决策过程。在这样一个*白盒*环境中，不需要通过智能体和环境的大量交互来学习，可以直接用动态规划求解状态价值函数。通常也只适用于有限马尔可夫决策过程，即状态空间和动作空间是离散且有限的。

## 价值迭代

### 价值迭代过程

对于一个动作空间和状态空间有限的MDP
$$
|S|<\infty,|A|<\infty
$$
1. 对每个状态s,初始化V(s)=0

2. 重复以下过程直到收敛{

   对每个状态,更新
   $$
   V(s)=R(s)+\max_{a\in A}\gamma\sum_{s^{\prime}\in S}P_{sa}(s^{\prime})V(s^{\prime})
   $$
   }	（更新V(s)直到收敛）

### 同步 vs. 异步价值迭代

**同步**价值迭代会储存两份价值函数的拷贝

1. 对𝑆中的所有状态s

   $$
   V_{new}(s)\leftarrow\max_{a\in A}\left(R(s)+\gamma\sum_{s^{\prime}\in S}P_{sa}(s^{\prime})V_{old}(s^{\prime})\right)
   $$

2. 更新$ V_{old}(s)\leftarrow V_{new}(s) $

需要存放两份价值函数，需要的内存是两倍



异步价值迭代只储存一份价值函数

- 对𝑆中的所有状态s

$$
V(s)\leftarrow\max_{a\in A}\left(R(s)+\gamma\sum_{s'\in S}P_{sa}(s')V(s')\right)
$$

更新过程中将存在一些紊乱。



## 策略迭代

对于一个动作空间和状态空间有限的MDP

$$
|S|<\infty,|A|<\infty
$$
策略迭代过程

1. 随机初始化策略 𝜋

2. 重复以下过程直到收敛{ 
   a. 根据 𝜋 计算 $ 𝑉 ≔ 𝑉^𝜋 $
   b. 对每个状态，更新
   $$
   \pi(s)=arg\max_{a\in A}\sum_{s^{\prime}\in S}P_{sa}(s^{\prime})V(s^{\prime})
   $$
   }	（通过价值函数得到最新的策略，V(s)是个辅助的作用）

a步更新价值函数更耗时间。

<img src="/images/image-20231010193833579.png" alt="image-20231010193833579" style="zoom:50%;" />

## [价值迭代](#价值迭代) vs. [策略迭代](#策略迭代)

1. 价值迭代是贪心更新法
2. 策略迭代中，用Bellman等式更新价值函数代价很大
3. 对于空间较小的MDP，策略迭代通常很快收敛
4. 对于空间较大的MDP，价值选代更实用(效率更高)
5. 如果没有状态转移循环，最好使用价值迭代

## 基于模型的强化学习

目前我们关注在给出一个已知MDP模型后：（也就是说，状态转移$𝑃_{𝑠𝑎}(𝑠^′)$和奖励函数𝑅(𝑠)明确给定后）

- 计算最优价值函数
- 学习最优策略

在实际问题中，状态转移和奖励函数一般不是明确给出的.

比如，我们只看到了一些episodes

$$
\begin{aligned}&\text{Episode1:}\quad s_0^{(1)}\xrightarrow{a_0^{(1)},R(s_0)^{(1)}}s_1^{(1)}\xrightarrow{a_1^{(1)},R(s_1)^{(1)}}s_2^{(1)}\xrightarrow{a_2^{(1)},R(s_2)^{(1)}}s_3^{(1)}\cdots s_r^{(1)}\\&\text{Episode2:}\quad s_0^{(2)}\xrightarrow{a_0^{(2)},R(s_0)^{(2)}}s_1^{(2)}\xrightarrow{a_1^{(2)},R(s_1)^{(2)}}s_2^{(2)}\xrightarrow{a_2^{(2)},R(s_2)^{(2)}}s_3^{(2)}\cdots s_r^{(2)}\end{aligned}
$$

> 在强化学习中，episode（也称为轮次或回合）指的是agent在环境中进行一系列动作和观察的完整序列。它从起始状态开始，经过一系列动作和环境反馈，直到达到终止状态为止。在每个episode结束时，agent可以根据其在该episode中的表现进行学习和更新策略。每个episode可以看作是一次完整的训练过程，agent通过与环境的交互来提升自己的性能。

基于模型的强化学习的一个基本思路是说，先根据episodes先回去学习一个MDP模型，然后我再用MDP模型的这样一些动态规划算法把最优的策略和价值函数给学出来。

所以说，如果是要从经验当中去学习MDP模型的话，其实他就是学习 状态转移的概率 和  reward function奖励函数𝑅(𝑠)。

学习状态转移概率$𝑃_{𝑠𝑎}(𝑠^′)$
$$
P_{sa}(s^{\prime})=\frac{\text{在}s\text{下采取动作}a\text{并转移到s'的次数}}{\text{在}s\text{下采取动作}a\text{的次数}}
$$
学习奖励函数𝑅(𝑠) ，也就是立即奖赏期望



#### 算法

1. 随机初始化策略𝜋

2. 重复以下过程直到收敛 {

   a) 在MDP中执行𝜋，收集经验数据

   b) 使用MDP中的累积经验更新对𝑃𝑠𝑎和𝑅的估计

   c) 利用对$𝑃_{𝑠𝑎}$和𝑅的估计执行价值迭代，得到新的估计价值函数𝑉

   d) 根据𝑉更新策略𝜋为贪心策略

   }

> 以上是基于模型的强化学习。一个白盒环境给定的情况下，可用动态规划的方法（值迭代和策略迭代）求解最优策略。如果环境是黑盒的，可以根据统计信息来拟合出动态环境𝑃和𝑅，然后做动态规划求解最优策略。另一种解决方式是不学习MDP，从经验中直接学习价值函数和策略。详细见于下一节。







## 悬崖漫步环境（Cliff Walking）

悬崖漫步是一个非常经典的强化学习环境，它要求一个智能体从起点出发，避开悬崖行走，最终到达目标位置。



